{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object):\n",
    "    def __init__(self, n_input, n_z, network_architecture, \n",
    "                 learning_rate=0.001, decoder_distribution='gaussian', \n",
    "                 reparameterization_trick_for_train=True, \n",
    "                 reparameterization_trick_for_gradients=True):\n",
    "        self.n_input = n_input\n",
    "        self.n_z = n_z\n",
    "        self.network_architecture = network_architecture\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decoder_distribution = decoder_distribution\n",
    "        self.reparameterization_trick_for_train = reparameterization_trick_for_train\n",
    "        self.reparameterization_trick_for_gradients = reparameterization_trick_for_gradients\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, n_input])\n",
    "        self.n_x = tf.cast(tf.shape(self.x)[1], tf.float32)\n",
    "        \n",
    "        self._create_network()\n",
    "        \n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        init = tf.initialize_all_variables()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    def _create_network(self):\n",
    "        self.weights = self._initialize_weights(**self.network_architecture)\n",
    "        \n",
    "        encoder_layer1 = tf.nn.softplus(tf.add(tf.matmul(self.x, self.weights['encoder']['h1']),\n",
    "                                               self.weights['encoder']['b1']))\n",
    "        encoder_layer2 = tf.nn.softplus(tf.add(tf.matmul(encoder_layer1, self.weights['encoder']['h2']), \n",
    "                                               self.weights['encoder']['b2']))\n",
    "        self.z_mean = tf.add(tf.matmul(encoder_layer2, self.weights['encoder']['out_mean']), \n",
    "                             self.weights['encoder']['out_mean_b'])\n",
    "        self.z_log_sigma_sq = tf.add(tf.matmul(encoder_layer2, self.weights['encoder']['out_log_sigma_sq']), \n",
    "                                     self.weights['encoder']['out_log_sigma_sq_b'])\n",
    "        if self.reparameterization_trick_for_train == True:\n",
    "            epsilon = tf.random_normal((tf.shape(self.x)[0], self.n_z), 0, 1, dtype=tf.float32)\n",
    "            self.z = tf.add(self.z_mean, tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq)), epsilon))\n",
    "        else:\n",
    "            self.z = tf.random_normal((tf.shape(self.x)[0], self.n_z), \n",
    "                                      self.z_mean, tf.sqrt(tf.exp(self.z_log_sigma_sq)))\n",
    "        \n",
    "        decoder_layer1 = tf.nn.softplus(tf.add(tf.matmul(self.z, self.weights['decoder']['h1']),\n",
    "                                               self.weights['decoder']['b1']))\n",
    "        decoder_layer2 = tf.nn.softplus(tf.add(tf.matmul(decoder_layer1, self.weights['decoder']['h2']), \n",
    "                                               self.weights['decoder']['b2']))\n",
    "        self.x_reconstruction = tf.sigmoid(tf.add(tf.matmul(decoder_layer2, self.weights['decoder']['out_mean']),\n",
    "                                                  self.weights['decoder']['out_mean_b']))\n",
    "        \n",
    "    def _initialize_weights(self, n_hidden_encoder_1, n_hidden_encoder_2, \n",
    "                           n_hidden_decoder_1, n_hidden_decoder_2):\n",
    "        weights = dict()\n",
    "        weights['encoder'] = {\n",
    "            'h1': tf.Variable(xavier_init(self.n_input, n_hidden_encoder_1)), \n",
    "            'h2': tf.Variable(xavier_init(n_hidden_encoder_1, n_hidden_encoder_2)), \n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_encoder_2, self.n_z)),\n",
    "            'out_log_sigma_sq': tf.Variable(xavier_init(n_hidden_encoder_2, self.n_z)),\n",
    "            \n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_encoder_1], dtype=tf.float32)), \n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_encoder_2], dtype=tf.float32)), \n",
    "            'out_mean_b': tf.Variable(tf.zeros([self.n_z], dtype=tf.float32)),\n",
    "            'out_log_sigma_sq_b': tf.Variable(tf.zeros([self.n_z], dtype=tf.float32))\n",
    "        }\n",
    "        weights['decoder'] = {\n",
    "            'h1': tf.Variable(xavier_init(self.n_z, n_hidden_decoder_1)), \n",
    "            'h2': tf.Variable(xavier_init(n_hidden_decoder_1, n_hidden_decoder_2)), \n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_encoder_2, self.n_input)),\n",
    "            \n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_decoder_1], dtype=tf.float32)), \n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_decoder_2], dtype=tf.float32)), \n",
    "            'out_mean_b': tf.Variable(tf.zeros([self.n_input], dtype=tf.float32))\n",
    "        }\n",
    "        return weights\n",
    "    \n",
    "    def _create_loss_optimizer(self):\n",
    "        if self.decoder_distribution == 'gaussian':\n",
    "            self.inference_log_density = -0.5*self.n_x*tf.log(2*np.pi) \\\n",
    "                                         - 0.5*tf.reduce_sum(tf.square(tf.sub(self.x_reconstruction, self.x)), 1)\n",
    "            self.recognition_log_density = -0.5*self.n_z*tf.log(2*np.pi) -0.5*tf.reduce_sum(self.z_log_sigma_sq, 1) \\\n",
    "                                           -0.5*tf.reduce_sum(tf.mul(tf.exp(-self.z_log_sigma_sq), \n",
    "                                                                     tf.square(tf.sub(self.z, self.z_mean))), 1)\n",
    "            if self.reparameterization_trick_for_train == True:\n",
    "                self.decoder_cost = 0.5*tf.reduce_sum(tf.square(tf.sub(self.x_reconstruction, self.x)), 1)\n",
    "            else:\n",
    "                self.decoder_cost = -tf.mul(self.inference_log_density, self.recognition_log_density)\n",
    "            \n",
    "            if self.reparameterization_trick_for_gradients == True:\n",
    "                self.decoder_cost_for_gradients = 0.5 \\\n",
    "                                                  * tf.reduce_sum(tf.square(tf.sub(self.x_reconstruction, self.x)), 1)\n",
    "            else:\n",
    "                self.decoder_cost_for_gradients = -tf.mul(self.inference_log_density, self.recognition_log_density)\n",
    "        elif self.decoder_distribution == 'bernoulli':\n",
    "            self.decoder_cost = -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstruction)\n",
    "                                               + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstruction), 1)\n",
    "        else:\n",
    "            raise ValueError('Unsupported decoder distribution!')\n",
    "        self.encoder_cost = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "                                            - tf.square(self.z_mean) \n",
    "                                            - tf.exp(self.z_log_sigma_sq), 1)\n",
    "        self.cost = tf.reduce_mean(self.decoder_cost + self.encoder_cost)\n",
    "        self.cost_for_gradients = tf.reduce_mean(self.decoder_cost_for_gradients + self.encoder_cost)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.minimizer = self.optimizer.minimize(self.cost)\n",
    "        \n",
    "        self.compute_gradients = self.optimizer.compute_gradients(self.cost_for_gradients)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        self.sess.run(self.minimizer, feed_dict={self.x: X})\n",
    "    \n",
    "    def get_gradients(self, X):\n",
    "        gradients = self.sess.run(self.compute_gradients, feed_dict={self.x: X})\n",
    "        flat_gradients = np.array([])\n",
    "        for grad in gradients:\n",
    "            flat_gradients = np.append(flat_gradients, grad[0].flatten())\n",
    "        return flat_gradients\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "    \n",
    "    def generate(self, z=None):\n",
    "        if z is None:\n",
    "            z = np.random.normal(size=self.n_z)\n",
    "        return self.sess.run(self.x_reconstruction, feed_dict={self.z: z})\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        return self.sess.run(self.x_reconstruction, feed_dict={self.x: X})\n",
    "    \n",
    "    def loss(self, X):\n",
    "        return self.sess.run(self.cost, feed_dict={self.x: X})\n",
    "    \n",
    "    def decoder_loss(self, X):\n",
    "        return self.sess.run(self.decoder_cost, feed_dict={self.x: X})\n",
    "    \n",
    "    def encoder_loss(self, X):\n",
    "        return self.sess.run(self.encoder_cost, feed_dict={self.x: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = mnist.train.num_examples\n",
    "n_input = 784\n",
    "n_z = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "network_architecture = {\n",
    "    'n_hidden_encoder_1': 100,\n",
    "    'n_hidden_encoder_2': 100,\n",
    "    'n_hidden_decoder_1': 100,\n",
    "    'n_hidden_decoder_2': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(data, n_samples, n_input, n_z, batch_size, \n",
    "          network_architecture, learning_rate, decoder_distribution, \n",
    "          reparameterization_trick_for_train, reparameterization_trick_for_gradients,\n",
    "          training_epochs=10, display_step=5, variance_display_step=5):\n",
    "    vae = VariationalAutoencoder(n_input, n_z, network_architecture, learning_rate, \n",
    "                                 decoder_distribution, reparameterization_trick_for_train, \n",
    "                                 reparameterization_trick_for_gradients)\n",
    "    for epoch in xrange(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples / batch_size)\n",
    "        for i in xrange(total_batch):\n",
    "            batch_xs, _ = data.train.next_batch(batch_size)\n",
    "            vae.partial_fit(batch_xs)\n",
    "            cost = vae.loss(batch_xs)\n",
    "            avg_cost += cost / n_samples * batch_size\n",
    "        \n",
    "        if epoch % display_step == 0:\n",
    "            print('Epoch: {:04d}, cost = {:.9f}, test cost = {:.9f}' \\\n",
    "                  .format(epoch+1, avg_cost, vae.loss(data.test.images)))\n",
    "            \n",
    "        if epoch % variance_display_step == 0:\n",
    "            gradients = []\n",
    "            for _ in xrange(100):\n",
    "                gradient = vae.get_gradients(batch_xs)\n",
    "                gradients.append(gradient)\n",
    "            gradients = np.array(gradients)\n",
    "            gradient_variance = np.linalg.norm(gradients - gradients.mean(axis=0)) / 100\n",
    "            print('Epoch: {:04d}, gradient variance = {:.9f}'.format(epoch+1, gradient_variance))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with reparameterization trick and compute gradients with reparameterization trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, cost = 27.249713381, test cost = 25.881767273\n",
      "Epoch: 0001, gradient variance = 0.123692751\n",
      "Epoch: 0006, cost = 21.156966511, test cost = 20.803092957\n",
      "Epoch: 0011, cost = 20.022824222, test cost = 19.922349930\n",
      "Epoch: 0016, cost = 19.610593064, test cost = 19.531904221\n",
      "Epoch: 0021, cost = 19.393669718, test cost = 19.426069260\n",
      "Epoch: 0026, cost = 19.171127066, test cost = 19.149215698\n",
      "Epoch: 0026, gradient variance = 0.509512763\n",
      "Epoch: 0031, cost = 19.035913082, test cost = 18.996868134\n",
      "Epoch: 0036, cost = 18.920415157, test cost = 18.909206390\n",
      "Epoch: 0041, cost = 18.828198045, test cost = 18.834566116\n",
      "Epoch: 0046, cost = 18.740361096, test cost = 18.779804230\n",
      "Epoch: 0051, cost = 18.682113856, test cost = 18.713949203\n",
      "Epoch: 0051, gradient variance = 0.573750052\n",
      "Epoch: 0056, cost = 18.628590383, test cost = 18.683971405\n",
      "Epoch: 0061, cost = 18.579649159, test cost = 18.593568802\n",
      "Epoch: 0066, cost = 18.533772153, test cost = 18.635084152\n",
      "Epoch: 0071, cost = 18.495735203, test cost = 18.572805405\n",
      "Epoch: 0076, cost = 18.470369915, test cost = 18.493724823\n",
      "Epoch: 0076, gradient variance = 0.572760978\n"
     ]
    }
   ],
   "source": [
    "vae = train(mnist, n_samples, n_input, n_z, \n",
    "            batch_size, network_architecture, \n",
    "            learning_rate, decoder_distribution='gaussian', \n",
    "            reparameterization_trick_for_train=True, \n",
    "            reparameterization_trick_for_gradients=True, \n",
    "            training_epochs=76, variance_display_step=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with reparameterization trick, but compute gradients with log-derivative trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, cost = 27.249713381, test cost = 25.881767273\n",
      "Epoch: 0001, gradient variance = 3.698353952\n",
      "Epoch: 0006, cost = 21.156966511, test cost = 20.803092957\n",
      "Epoch: 0011, cost = 20.022824222, test cost = 19.922349930\n",
      "Epoch: 0016, cost = 19.610593064, test cost = 19.531904221\n",
      "Epoch: 0021, cost = 19.393669718, test cost = 19.426069260\n",
      "Epoch: 0026, cost = 19.171127066, test cost = 19.149215698\n",
      "Epoch: 0026, gradient variance = 12.217070787\n",
      "Epoch: 0031, cost = 19.035913082, test cost = 18.996868134\n",
      "Epoch: 0036, cost = 18.920415157, test cost = 18.909206390\n",
      "Epoch: 0041, cost = 18.828198045, test cost = 18.834566116\n",
      "Epoch: 0046, cost = 18.740361096, test cost = 18.779804230\n",
      "Epoch: 0051, cost = 18.682113856, test cost = 18.713949203\n",
      "Epoch: 0051, gradient variance = 13.589430122\n",
      "Epoch: 0056, cost = 18.628590383, test cost = 18.683971405\n",
      "Epoch: 0061, cost = 18.579649159, test cost = 18.593568802\n",
      "Epoch: 0066, cost = 18.533772153, test cost = 18.635084152\n",
      "Epoch: 0071, cost = 18.495735203, test cost = 18.572805405\n",
      "Epoch: 0076, cost = 18.470369915, test cost = 18.493724823\n",
      "Epoch: 0076, gradient variance = 13.353386582\n"
     ]
    }
   ],
   "source": [
    "vae = train(mnist, n_samples, n_input, n_z, \n",
    "            batch_size, network_architecture, \n",
    "            learning_rate, decoder_distribution='gaussian', \n",
    "            reparameterization_trick_for_train=True, \n",
    "            reparameterization_trick_for_gradients=False, \n",
    "            training_epochs=76, variance_display_step=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with log-derivative trick and compute gradients with log-derivative trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, cost = -90745.160958807, test cost = -92366.554687500\n",
      "Epoch: 0001, gradient variance = 0.008425934\n",
      "Epoch: 0006, cost = -92418.387428977, test cost = -92386.843750000\n",
      "Epoch: 0011, cost = -92433.870312500, test cost = -92427.234375000\n",
      "Epoch: 0016, cost = -92422.569133523, test cost = -92429.914062500\n",
      "Epoch: 0021, cost = -92436.126491477, test cost = -92467.539062500\n",
      "Epoch: 0026, cost = -92414.301704545, test cost = -92460.054687500\n",
      "Epoch: 0026, gradient variance = 0.000082183\n",
      "Epoch: 0031, cost = -92435.803494318, test cost = -92393.507812500\n",
      "Epoch: 0036, cost = -92421.430298295, test cost = -92424.023437500\n",
      "Epoch: 0041, cost = -92420.640397727, test cost = -92450.015625000\n",
      "Epoch: 0046, cost = -92444.738536932, test cost = -92429.054687500\n",
      "Epoch: 0051, cost = -92472.283664773, test cost = -92447.039062500\n",
      "Epoch: 0051, gradient variance = 0.000041690\n",
      "Epoch: 0056, cost = -92449.946107955, test cost = -92453.953125000\n",
      "Epoch: 0061, cost = -92416.251889205, test cost = -92393.328125000\n",
      "Epoch: 0066, cost = -92417.370681818, test cost = -92405.601562500\n",
      "Epoch: 0071, cost = -92418.838181818, test cost = -92443.695312500\n",
      "Epoch: 0076, cost = -92409.018252841, test cost = -92433.281250000\n",
      "Epoch: 0076, gradient variance = 0.000040550\n"
     ]
    }
   ],
   "source": [
    "vae = train(mnist, n_samples, n_input, n_z, \n",
    "            batch_size, network_architecture, \n",
    "            learning_rate, decoder_distribution='gaussian', \n",
    "            reparameterization_trick_for_train=False, \n",
    "            reparameterization_trick_for_gradients=False, \n",
    "            training_epochs=76, variance_display_step=25)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
