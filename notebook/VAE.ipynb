{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a dataset $X = \\{x^{(i)}\\}_{i=1}^N$ consisting of N i.i.d. samples. We assume that the data are generated from parametric family of distributions $p_{\\theta^*}(x)$ and we introduce the generative model $p_{\\theta^*}(x, z) = p_{\\theta^*}(x|z)p_{\\theta^*}(z)$ where $z$ is an unobserved continuous random variable. The true parameters $\\theta^*$ and the values of the latent variables $z^{(i)}$ are unknown to us. \n",
    "\n",
    "It is worth noting that we are interested in a general algorithm that works efficiently in the case of:\n",
    "* intractability of the marginal likelihood $p_{\\theta}(x) = \\int p_{\\theta}(x|z)p_{\\theta}(z)dz$ and the true posterior density $p_{\\theta}(z|x) = \\dfrac{p_{\\theta}(x|z)p_{\\theta}(z)}{p_{\\theta}(x)}$;\n",
    "* scalability for a large dataset .\n",
    "\n",
    "Our purpose is to solve the following three problems:\n",
    "* efficient approximate ML or MAP estimation for the parameters $\\theta$; \n",
    "* efficient approximate posterior inference of the latent variable $p_{\\theta}(z|x)$; \n",
    "* efficient approximate marginal inference of the variable x. \n",
    "\n",
    "The algorithm which solves the above problems was proposed by D. Kingma and Prof. Dr. M. Welling in the paper [Auto-Encoding Variational Bayes](http://arxiv.org/abs/1312.6114). At first authors introduce a recognition model $q_{\\varphi}(z|x)$: an approximation to the intractable true posterior $p_{\\theta}(z|x)$. After Kingma et al. introduce a method for learning the recognition model parameters $\\varphi$ jointly with the generative model parameters $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea is to use the variational lower bound of the marginal likelihood \n",
    "$\\ln p_{\\theta}(x^{(1)}, \\dots, x^{(N)})= \\sum_{i=1}^N\\ln p_{\\theta}(x^{(i)})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ln p_{\\theta}(x^{(i)}) = D_{KL}(q_{\\varphi}(z|x^{(i)})||p_{\\theta}(z|x^{(i)})) + L(\\theta, \\varphi; x^{(i)}) \\quad \\Rightarrow \n",
    "$$\n",
    "$$\n",
    "\\Rightarrow \\quad \\ln p_{\\theta}(x^{(i)}) \\geqslant L(\\theta, \\varphi; x^{(i)}) = \\mathbf{E}_{q_{\\varphi}(z|x)}\\left(-\\ln q_{\\varphi}(z|x) + \\ln p_{\\theta}(x, z)\\right) = -D_{KL}(q_{\\varphi}(z|x^{(i)})||p_{\\theta}(z)) + \\mathbf{E}_{q_{\\varphi}(z|x^{(i)})}(\\ln p_{\\theta}(x^{(i)}|z))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to maximize the lower bound $L(\\theta, \\varphi; x^{(i)})$ w.r.t. both the variational parameters $\\varphi$ and the generative parameters $\\theta$. However, we have some difficulties with the gradient of the lower bound w.r.t. $\\varphi$. The usual Monte Carlo gradient estimator is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\varphi}\\mathbf{E}_{q_{\\varphi}(z)}\\left[f(z, \\varphi)\\right] = \\mathbf{E}_{q_{\\varphi}(z)}\n",
    "\\left[\\nabla_{\\varphi}f(z, \\varphi)\\right] + \\mathbf{E}_{q_{\\varphi}(z)}\\left[f(z, \\varphi)\\nabla_{\\varphi}\\ln q_{\\varphi}(z)\\right] \\approx\n",
    "\\dfrac{1}{L}\\sum_{l=1}^L(\\nabla_{\\varphi}f(z_l, \\varphi) + f(z_l, \\varphi)\\nabla_{\\varphi}\\ln q_{\\varphi}(z_l))\\quad\n",
    "\\text{ where } \\quad z_l \\sim q_{\\varphi}(z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the term $f(z_l, \\varphi)\\nabla_{\\varphi}\\ln q_{\\varphi}(z_l)$ in our gradient estimator exhibits very high variance and is impractical for our purposes. But in case of the continuous latent variable $z$ with certain mild conditions for a chosen approximate posterior $q_{\\theta}(z|x)$ we can utilize the reparameterization trick. The idea is simple. If it is possible to express the variable $z$ as a deterministic variable $z = g_{\\varphi}(\\varepsilon, x)$ where $\\varepsilon$ is a random variable with independent marginal $p(\\varepsilon)$ and $g_{\\varphi}(.)$ is some vector-valued function parameterized by $\\varphi$, then the following is true: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int q_{\\varphi}(z|x)f(z, \\varphi)dz = \\int p(\\varepsilon)f(z, \\varphi) d\\varepsilon = \\int p(\\varepsilon)f(g_{\\varphi}(\\varepsilon, x), \\varphi)d\\varepsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we obtain more robust Monte Carlo gradient estimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\varphi}\\mathbf{E}_{q_{\\varphi}(z|x)}\\left[f(z, \\varphi)\\right] = \\nabla_{\\varphi}\\mathbf{E}_{p(\\varepsilon)}\\left[f(g_{\\varphi}(\\varepsilon, x), \\varphi)\\right] = \\mathbf{E}_{p(\\varepsilon)}\\left[\\nabla_{\\varphi}f(g_{\\varphi}(\\varepsilon, x), \\varphi)\\right] \\approx\n",
    "\\dfrac{1}{L}\\sum_{l=1}^L\\nabla_{\\varphi}f(g_{\\varphi}(\\varepsilon_l, x), \\varphi)\\quad\n",
    "\\text{ where } \\quad \\varepsilon_l \\sim p(\\varepsilon)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this technique to the variational lower bound, yielding the generic Stochastic Gradient Variational Bayes (SGVB) estimator $\\hat{L}(\\theta, \\varphi; x) \\approx L(\\theta, \\varphi; x)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{L}(\\theta, \\varphi; x) = \\dfrac{1}{L}\\sum_{i=1}^L\\ln p_{\\theta}(x, z_l) - \\ln q_{\\varphi}(z_l|x) \\quad\n",
    "\\text{ where } \\quad z_l = g_{\\varphi}(\\varepsilon_l, x) \\quad \\text{ and } \\quad \\varepsilon_l \\sim p(\\varepsilon). \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object):\n",
    "    def __init__(self, n_input, n_z, network_architecture, \n",
    "                 learning_rate=0.001, decoder_distribution='gaussian'):\n",
    "        self.n_input = n_input\n",
    "        self.n_z = n_z\n",
    "        self.network_architecture = network_architecture\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decoder_distribution = decoder_distribution\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, n_input])\n",
    "        \n",
    "        self._create_network()\n",
    "        \n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        init = tf.initialize_all_variables()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    def _create_network(self):\n",
    "        self.weights = self._initialize_weights(**self.network_architecture)\n",
    "        \n",
    "        encoder_layer1 = tf.nn.softplus(tf.add(tf.matmul(self.x, self.weights['encoder']['h1']),\n",
    "                                               self.weights['encoder']['b1']))\n",
    "        encoder_layer2 = tf.nn.softplus(tf.add(tf.matmul(encoder_layer1, self.weights['encoder']['h2']), \n",
    "                                               self.weights['encoder']['b2']))\n",
    "        self.z_mean = tf.add(tf.matmul(encoder_layer2, self.weights['encoder']['out_mean']), \n",
    "                             self.weights['encoder']['out_mean_b'])\n",
    "        self.z_log_sigma_sq = tf.add(tf.matmul(encoder_layer2, self.weights['encoder']['out_log_sigma_sq']), \n",
    "                                     self.weights['encoder']['out_log_sigma_sq_b'])\n",
    "        epsilon = tf.random_normal((tf.shape(self.x)[0], self.n_z), 0, 1, dtype=tf.float32)\n",
    "        self.z = tf.add(self.z_mean, tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq)), epsilon))\n",
    "        \n",
    "        decoder_layer1 = tf.nn.softplus(tf.add(tf.matmul(self.z, self.weights['decoder']['h1']),\n",
    "                                               self.weights['decoder']['b1']))\n",
    "        decoder_layer2 = tf.nn.softplus(tf.add(tf.matmul(decoder_layer1, self.weights['decoder']['h2']), \n",
    "                                               self.weights['decoder']['b2']))\n",
    "        self.x_reconstruction = tf.sigmoid(tf.add(tf.matmul(decoder_layer2, self.weights['decoder']['out_mean']),\n",
    "                                                  self.weights['decoder']['out_mean_b']))\n",
    "        \n",
    "    def _initialize_weights(self, n_hidden_encoder_1, n_hidden_encoder_2, \n",
    "                           n_hidden_decoder_1, n_hidden_decoder_2):\n",
    "        weights = dict()\n",
    "        weights['encoder'] = {\n",
    "            'h1': tf.Variable(xavier_init(self.n_input, n_hidden_encoder_1)), \n",
    "            'h2': tf.Variable(xavier_init(n_hidden_encoder_1, n_hidden_encoder_2)), \n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_encoder_2, self.n_z)),\n",
    "            'out_log_sigma_sq': tf.Variable(xavier_init(n_hidden_encoder_2, self.n_z)),\n",
    "            \n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_encoder_1], dtype=tf.float32)), \n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_encoder_2], dtype=tf.float32)), \n",
    "            'out_mean_b': tf.Variable(tf.zeros([self.n_z], dtype=tf.float32)),\n",
    "            'out_log_sigma_sq_b': tf.Variable(tf.zeros([self.n_z], dtype=tf.float32))\n",
    "        }\n",
    "        weights['decoder'] = {\n",
    "            'h1': tf.Variable(xavier_init(self.n_z, n_hidden_decoder_1)), \n",
    "            'h2': tf.Variable(xavier_init(n_hidden_decoder_1, n_hidden_decoder_2)), \n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_encoder_2, self.n_input)),\n",
    "            \n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_decoder_1], dtype=tf.float32)), \n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_decoder_2], dtype=tf.float32)), \n",
    "            'out_mean_b': tf.Variable(tf.zeros([self.n_input], dtype=tf.float32))\n",
    "        }\n",
    "        return weights\n",
    "    \n",
    "    def _create_loss_optimizer(self):\n",
    "        if self.decoder_distribution == 'gaussian':\n",
    "            self.decoder_cost = 0.5 * tf.reduce_sum(tf.square(tf.sub(self.x_reconstruction, self.x)), 1)\n",
    "        elif self.decoder_distribution == 'bernoulli':\n",
    "            self.decoder_cost = -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstruction)\n",
    "                                               + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstruction), 1)\n",
    "        else:\n",
    "            raise ValueError('Unsupported decoder distribution!')\n",
    "        self.encoder_cost = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "                                            - tf.square(self.z_mean) \n",
    "                                            - tf.exp(self.z_log_sigma_sq), 1)\n",
    "        self.cost = tf.reduce_mean(self.decoder_cost + self.encoder_cost)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        opt, cost = self.sess.run((self.optimizer, self.cost), \n",
    "                                  feed_dict={self.x: X})\n",
    "        return cost\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "    \n",
    "    def generate(self, z=None):\n",
    "        if z is None:\n",
    "            z = np.random.normal(size=self.n_z)\n",
    "        return self.sess.run(self.x_reconstruction, feed_dict={self.z: z})\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        return self.sess.run(self.x_reconstruction, feed_dict={self.x: X})\n",
    "    \n",
    "    def loss(self, X):\n",
    "        return self.sess.run(self.cost, feed_dict={self.x: X})\n",
    "    \n",
    "    def decoder_loss(self, X):\n",
    "        return self.sess.run(self.decoder_cost, feed_dict={self.x: X})\n",
    "    \n",
    "    def encoder_loss(self, X):\n",
    "        return self.sess.run(self.encoder_cost, feed_dict={self.x: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = mnist.train.num_examples\n",
    "n_input = 784\n",
    "n_z = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "network_architecture = {\n",
    "    'n_hidden_encoder_1': 500,\n",
    "    'n_hidden_encoder_2': 500,\n",
    "    'n_hidden_decoder_1': 500,\n",
    "    'n_hidden_decoder_2': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(data, n_samples, n_input, n_z, batch_size, \n",
    "          network_architecture, learning_rate, decoder_distribution,\n",
    "          training_epochs=10, display_step=5):\n",
    "    vae = VariationalAutoencoder(n_input, n_z, network_architecture, \n",
    "                                 learning_rate, decoder_distribution)\n",
    "    for epoch in xrange(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples / batch_size)\n",
    "        for i in xrange(total_batch):\n",
    "            batch_xs, _ = data.train.next_batch(batch_size)\n",
    "            vae.partial_fit(batch_xs)\n",
    "            cost = vae.loss(batch_xs)\n",
    "            avg_cost += cost / n_samples * batch_size\n",
    "        \n",
    "        if epoch % display_step == 0:\n",
    "            print('Epoch: {:04d}, cost = {:.9f}, test cost = {:.9f}' \\\n",
    "                  .format(epoch+1, avg_cost, vae.loss(data.test.images)))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training in Gaussian case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, cost = 26.830678402, test cost = 26.169908524\n",
      "Epoch: 0006, cost = 21.699161960, test cost = 21.548978806\n",
      "Epoch: 0011, cost = 20.347310236, test cost = 20.237585068\n",
      "Epoch: 0016, cost = 19.873470220, test cost = 19.883232117\n",
      "Epoch: 0021, cost = 19.648401035, test cost = 19.668369293\n",
      "Epoch: 0026, cost = 19.455578367, test cost = 19.484958649\n",
      "Epoch: 0031, cost = 19.262910617, test cost = 19.314926147\n",
      "Epoch: 0036, cost = 19.053380540, test cost = 19.132003784\n",
      "Epoch: 0041, cost = 18.906792235, test cost = 19.012399673\n",
      "Epoch: 0046, cost = 18.818410835, test cost = 18.907796860\n",
      "Epoch: 0051, cost = 18.739035298, test cost = 18.829015732\n",
      "Epoch: 0056, cost = 18.667027668, test cost = 18.784650803\n",
      "Epoch: 0061, cost = 18.586155396, test cost = 18.728923798\n",
      "Epoch: 0066, cost = 18.539185541, test cost = 18.650930405\n",
      "Epoch: 0071, cost = 18.467723257, test cost = 18.659910202\n"
     ]
    }
   ],
   "source": [
    "vae = train(mnist, n_samples, n_input, n_z, \n",
    "            batch_size, network_architecture, \n",
    "            learning_rate, 'gaussian', training_epochs=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training in Bernoulli case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, cost = 174.010542797, test cost = 139.077346802\n",
      "Epoch: 0006, cost = 108.510390348, test cost = 107.655998230\n",
      "Epoch: 0011, cost = 103.943840166, test cost = 104.144309998\n",
      "Epoch: 0016, cost = 101.791891369, test cost = 102.287849426\n",
      "Epoch: 0021, cost = 100.329291770, test cost = 101.443397522\n",
      "Epoch: 0026, cost = 99.015549094, test cost = 100.691474915\n",
      "Epoch: 0031, cost = 98.122421639, test cost = 99.742271423\n",
      "Epoch: 0036, cost = 97.364934665, test cost = 99.391883850\n",
      "Epoch: 0041, cost = 96.791749337, test cost = 98.885337830\n",
      "Epoch: 0046, cost = 96.290269567, test cost = 98.671485901\n",
      "Epoch: 0051, cost = 95.868905986, test cost = 98.590072632\n",
      "Epoch: 0056, cost = 95.555079054, test cost = 98.299934387\n",
      "Epoch: 0061, cost = 95.175326968, test cost = 97.915428162\n",
      "Epoch: 0066, cost = 94.922360243, test cost = 97.979713440\n",
      "Epoch: 0071, cost = 94.625248774, test cost = 97.754158020\n"
     ]
    }
   ],
   "source": [
    "vae = train(mnist, n_samples, n_input, n_z, \n",
    "            batch_size, network_architecture, \n",
    "            learning_rate, 'bernoulli', training_epochs=75)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
